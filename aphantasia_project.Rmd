---
output:
  pdf_document:
    toc: yes
    toc_depth: 5
    number_sections: yes
    extra_dependencies: ["float"]
    keep_md: yes
    fig_height: 8
    fig_width: 10
    df_print: kable
    includes:
      in_header: "z_preamble.tex"
      before_body: "z_beforebody.tex"
  html_document:
    toc: yes
    toc_depth: '5'
    df_print: kable
lang: "fr"
fontsize: 12pt
linestretch: 1.5
geometry: margin=1in
indent: TRUE
documentclass: article
# classoption:
# - twocolumn
bibliography: references.bib
csl: apa.csl
---
\newpage
```{r, include=FALSE, cache=FALSE}
# chargement du code source et reglages
knitr::read_chunk('./scripts/aphantasia_source.R')
knitr::opts_chunk$set(message=FALSE,
                      warning=FALSE,
                      echo=FALSE,
                      out.height = "50%",
                      out.width = "80%",
                      # fig.pos = "H",
                      fig.align = "center",
                      out.extra = ""
                      )
```

```{r,setup, echo=FALSE}
# chargement des donnees sans output
```

<!-- Le manuscrit est divisé en plusieurs documents pour faciliter les modifications locales - ceux-ci sont dans le dossier du Drive. L'output complet du R Markdown (le pdf final) est en pdf dans le Drive, avec le même nom que ce fichier. Les GDoc de ce Drive servent à éditer le texte rédigé du document final : le code R sous-jacent et les analyses des données seront partagées et travaillées sur GitHub, lié localement à RStudio. Le repository GitHub en question : https://github.com/m-delem/aphantasia_project.git -->

Test pour référencer une figure, ici la Figure \ref{correlation_matrix}, ou encore la Figure \ref{mixed_matrix}.

<!-- Introduction -->
```{r, child="1_intro.Rmd"}
```

\newpage
<!-- Méthodologie -->
```{r, child="2_experience.Rmd"}
```

\newpage
<!-- Discussion -->
```{r, child="3_discussion.Rmd"}
```

\newpage
# Références {-}

<div id="refs"></div>

\newpage
# Annexes {-}
Code complet des analyses :
```{r,echo=TRUE, eval=FALSE}
# 
# ---- Aphantasia Project - Source code ----------------------------------------
# 
# Delem, Fourment, Junoy, Leal De Almeida
# Email : m.delem@univ-lyon2.fr
# Last update : February 11, 2022

# ---- setup -------------------------------------------------------------------

# packages
shelf(
  MASS,       # functions and data frame ecosystem
  easystats,  # modelling, visualization and reporting ecosystem
  ez,         # analysis and visualization of factorial exp
  rstatix,    # pipe friendly statistical functions
  scale,      # scale functions
  corrr,      # correlations
  lme4,       # mixed models
  lmerTest,   # tests in lmer
  cluster,    # cluster analysis
  factoextra, # multivariate data analysis visualization
  GGally,     # more ggplot2 plots
  ggpubr,     # publication plots
  ggradar,    # radar charts
  ggraph,     # auto graph layout
  igraph,     # network graphs
)

# global theme
theme_set(theme_bw(base_size = 14, base_family = "serif"))

# random seed
set.seed(14051998)

# Simulation des donnees

# definition des variables et groupes
# groupe non-aphantasique
Non_A <- data.frame(
  name = c("OSIQ_O", "OSIQ_S", "VVIQ",
           "Raven", "Simili", "Wason",
           "Empan_MDT", "WCST", "Lecture",
           "Corsi","MRT", "SRI"),
  mean = c(54.6,  46.2,  63.8,
           20.9,  37.8,  32.2,
           6.43,  32.1,  50.2,
           5.81,  16.5,  35.7),
    sd = c(8.45,  9.54,  9.67,
           5.34,  4.25,  3.78,
           2.12,  5.32,  8.89,
           1.87,  3.54,  6.23),
  group = ("Non_A") %>% factor(),
  n_subjects = 200
)

# groupe aphantasique
Aph <- data.frame(
  name = c("OSIQ_O", "OSIQ_S", "VVIQ",
           "Raven", "Simili", "Wason",
           "Empan_MDT", "WCST", "Lecture",
           "Corsi","MRT", "SRI"),
  
  mean = c(32.5,  58.9,  30.2,
           23.6,  42.2,  36.1,
           7.53,  33.8,  48.4,
           6.8,   18.2,  38.5),
  
    sd = c(8.45,  9.54,  9.67,
           4.24,  6.15,  3.47,
           1.45,  2.62,  9.67,
           1.65,  5.78,  8.21),
  group = ("Aph") %>% factor(),
  n_subjects = 200
)

# dataset fusionné
variables <- bind_rows(Aph,Non_A)
rm(Aph,Non_A)

# liens variables-capacites cognitives
fmodel <- matrix(c ( .8,   0,  0,  0,  0, # OSIQ-O = img objet
                      0,  .9,  0,  0,  0, # OSIQ-S = img spatiale
                     .9,   0,  0,  0,  0, # VVIQ = img objet
                     .1,  .3, .8,  0,.05, # Raven = raisonnmt > img s/o > flex
                    -.2,   0, .6,  0, .1, # Simili = raisonnmt >  flex
                    -.1,   0, .3,  0,  0, # Wason = raisonnmt
                      0,   0,  0, .8,  0, # Empan = MDT 
                    -.1,   0, .2,  0, .6, # WCST = Flex > raisonnmt
                     .4,   0, .6,  0,  0, # Lecture = img objet > raisonnmt
                     .1,  .7,  0, .8,  0, # Corsi = MDT > img s
                     .2, .85,  0,  0,  0, # MRT = img s
                     .1,  .9,  0,  0,  0  # SRI = img s
                    ), 
                 nrow=12, ncol=5, byrow=TRUE)

# liens entre capacites cognitives
effect <- matrix(c (  1,-.1,-.1, .2,  0, # img o
                    -.1,  1, .3, .2,  0, # img s
                    -.1, .3,  1,  0, .2, # raisonnmt
                     .2, .2,  0,  1,  0, # MDT
                      0,  0, .2,  0,  1  # flex
                     ),
                 nrow=5, ncol=5, byrow=TRUE)

# fonction de simulation
simulation <- function(variables, fmodel, effect) {
  
  ### preparatifs ###
  n_variables <- dim(fmodel)[1] # notre nb de mesures/variables (rows)
  n_skills <- dim(fmodel)[2]    # les capacites sous-jacentes evaluees (columns)
  
  # matrice de poids des erreurs
  errorweight <- (1 - diag(fmodel %*% t(fmodel))) %>% 
                  abs() %>%   # necessaire pour la racine carree
                  sqrt() %>%  # doit avoir des arguments positifs
                  diag()      # recree une matrice diagonalisee
  
  # initialisation d'un dataframe vide
  data <- data.frame()
  
  ### simulation ###
  for (i in levels(variables$group)){   # on simule separement chaque groupe
    
    var_group = variables %>% filter(group == i) # donnees du groupe isolees
    n_subjects = var_group$n_subjects[1]         # nb de sujets dans le groupe
    group = i                                    # nom du groupe
    
    # generation de scores aleatoires normaux pour chaque capacite cognitive
    randomscores <- matrix(rnorm(n_subjects * (n_skills)),
                           nrow = n_subjects,
                           ncol = n_skills)
    # ponderation par la matrice d'effets = les scores sont desormais correles
    # entre eux
    skillscores <- randomscores %*% effect
    
    # genere les valeurs standardisees des mesures/variables grace a fmodel 
    observedscores <- skillscores %*% t(fmodel)
    
    # generation d'erreurs normales pour chaque mesure/variable
    randomerror <- matrix(rnorm(n_subjects * (n_variables)), 
                          nrow = n_subjects,
                          ncol = n_variables)
    # ponderation par notre matrice de poids des erreurs
    error <- randomerror %*% errorweight
    
    # nos mesures effectives = les valeurs reeles + une erreur standard
    measures <- observedscores + error
    
    # on cree un dataframe avec le nom de groupe
    data_group <- data.frame(measures) %>% 
      mutate(Group = group %>% factor())
    
    # ajout des valeurs reeles de moyenne et d'ecart-type pour chaque variable 
    # et renommage
    for (i in 1:length(var_group$name)){
      data_group[,i] = data_group[,i]*var_group$sd[i] + var_group$mean[i]
      colnames(data_group)[i] = var_group$name[i]
      }
    
    # fusion avec le dataframe complet
    data <- bind_rows(data,data_group)
    }
  
  # ajout d'id individuels et stats demographiques
  n = length(data[,1])  # nombre total de participants
  data <- data %>% 
    mutate(Subject_nr = row_number() %>% as.character(),
           Sex = (c("H","F") %>% rep(times = n/2) %>% factor()),
           Age = seq(from = 16, to = 55, by = 1) %>% sample(size = n, 
                                                            replace = TRUE)
           ) %>% 
    relocate(Subject_nr)

  # mission accomplished!
  return(data)  
  }

# la fonction est donc clefs en main
data <- simulation(variables,fmodel,effect)

# standardisation
# Paradoxalement on va defaire ce qu'on a construit avec les moyennes,
# en re-standardisant tous les scores sous forme de z-scores
data_scale <- data %>% 
  select(OSIQ_O:SRI) %>% 
  mutate(across(everything(), ~ scale(.x)))

# k-means clustering
data_kmeans <- kmeans(data_scale, 
                      centers = 4,
                      nstart = 100)

# on ajoute les clusters aux donnees des participants
data <- data %>% mutate(Cluster = data_kmeans$cluster %>% factor())

# Profils cognitifs sous-jacents des clusters
# on fusionne les scores des composantes proches
deep <- data %>% 
  mutate(Spatial_Img = OSIQ_S + Corsi + MRT + SRI,
         Object_Img = OSIQ_O + VVIQ,
         Reasoning = Raven + Simili + Wason,
         Executive = Empan_MDT + WCST + Lecture) %>% 
  select(Spatial_Img : Executive, Cluster) %>% 
  mutate(across(c(Spatial_Img : Executive), 
                ~ rescale(.x, to = c(0,1))),)

# on cree un dataset d'analyse avec les variables continues standardisees
# (pour plus tard)
data_analysis <- data %>% 
  mutate(across(c(-Subject_nr,-Group,-Sex,-Age,-Cluster), 
                ~ scale(.x))
  )

# ---- correlation_matrix ------------------------------------------------------
# package "correlations" de easystats
data_scale %>% 
  correlation(partial = TRUE) %>% 
  cor_sort() %>% 
  summary() %>% 
  visualisation_recipe(
    labs = list(title = "Correlations entre les variables mesurees")) %>% 
  plot() +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())

# ---- network_plot ------------------------------------------------------------
# graphe en reseau (corrr, igraph & ggraph)
data_scale %>%
  correlate() %>% 
  stretch %>% 
  filter(abs(r) >= .1) %>% 
  mutate(x = replace(x, x == "Empan_MDT", "Empan"),
         y = replace(y, y == "Empan_MDT", "Empan")) %>% 
  graph_from_data_frame(directed = FALSE) %>% 
  ggraph() +
    geom_edge_arc(strength = 0.1,
                  aes(edge_alpha = abs(r),
                      edge_width = abs(r),
                      colour = r)) +
    guides(edge_alpha = "none", edge_width = "none") +
    scale_edge_colour_gradientn(limits = c(-1, 1), 
                                colors = c("firebrick2", "dodgerblue2")) +
    geom_node_point(size = 20) +
    geom_node_text(aes(label = name,
                       family = "serif"),
                   colour = "white",
                   repel = FALSE) +
    theme_graph(base_family = "serif", base_size = 10) +
    #theme(legend.position = "none") +
    labs(title = "Correlations entre les variables mesurees")

# ---- mixed_matrix ------------------------------------------------------------
# matrice avec graphes et distributions
# GGally package
data_scale %>% 
  mutate(across(everything(), ~as.numeric(.x))) %>% 
  ggpairs(title = "Correlations et distributions des variables mesurees",
          lower = list(continuous = wrap("points", alpha = 0.2)),
          )

# ---- cluster_number ----------------------------------------------------------
data_scale %>% 
  fviz_nbclust(kmeans, method = "wss",
               linecolor = "white") +
  geom_vline(xintercept = 4, linetype = 2) +
  theme_bw(base_size = 14, base_family = "serif") +
  geom_line(aes(group = 1), color = "aquamarine2",size = 1.3) + 
  geom_point(group = 1, size = 3, color = "aquamarine4") +
  labs(title = "Nombre optimal de clusters (methode `Within Sum of Squares`)",
       x = "Nombre de clusters",
       y = "Total des Somme des Carres intra-clusters")

# ---- pca_variables -----------------------------------------------------------
data_scale %>% 
  prcomp(scale = TRUE) %>% 
  fviz_pca_var(repel = TRUE,
               col.var = "contrib",
               title = "Analyse en Composantes Principales des variables") + 
  theme_bw(base_size = 14, base_family = "serif") +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  labs(x = "Dimension 1 (40.7%)",
       y = "Dimension 2 (18.4%)")

# ---- k-means -----------------------------------------------------------------
data_kmeans %>% 
  fviz_cluster(
    data_scale,
    geom = "point",
    repel = TRUE,
    ellipse.type = "convex",
    shape = "circle", pointsize = 1.2,
    main = 
      "Representation des clusters selon les deux composantes principales",
    xlab = "Dimension 1 (40.7%)",
    ylab = "Dimension 2 (18.4%)",
    ) +
  theme_bw(base_size = 14, base_family = "serif")

# ---- cluster_repatition ------------------------------------------------------
# repartion des groupes par cluster
data %>% 
  ggbivariate(outcome = "Group",
              explanatory = "Cluster") +
  scale_fill_manual("Groupe", 
                    values = c("aquamarine2", "coral"),
                    labels = c("Aphantasiques","Non-Aphantasiques")) +
  labs(title = 
    "Repartition des aphantasiques et non-aphantasiques dans les clusters")

# ---- profiles_radar ----------------------------------------------------------
deep %>%
  group_by(Cluster) %>%  
  summarise(across(everything(),mean)) %>% 
  ggradar(base.size = 10,
          font.radar = "serif",
          values.radar = c("0","0.5","1"),
          grid.label.size = 4,
          axis.labels = c( "Imagerie Spatiale",
                           "Imagerie\n Objet", 
                           "Raisonnement",
                           "Fonctions\n Executives"),
          grid.min = 0, grid.mid = .5, grid.max = 1,
          label.gridline.min = FALSE,
          group.line.width = 1, group.point.size = 3,
          group.colours =,
          background.circle.transparency = .1,
          legend.title = "Clusters",
          legend.text.size = 12,
          legend.position = "bottom",
          plot.title = "Profils cognitifs des clusters identifies 
par partition non-supervisee (k-means)",
          fill = TRUE,
          fill.alpha = 0.1
  ) + 
  theme_bw(base_size = 14, base_family = "serif") +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
  #facet(facet.by = "Cluster")

# ---- profiles_lollipop -------------------------------------------------------
deep %>% 
  gather(key = variable, value = value, -Cluster) %>% 
  mutate(variable = replace(variable, variable == "Spatial_Img", "Imagerie\n Spatiale"),
         variable = replace(variable, variable == "Reasoning", "Raisonnement"),
         variable = replace(variable, variable == "Object_Img", "Imagerie\n Objet"),
         variable = replace(variable, variable == "Executive", "Fonctions\n Exectutives")) %>% 
  group_by(variable, Cluster) %>% 
  summarise(mean = mean(value)) %>% 
  ggdotchart(
    x = "variable",
    y = "mean",
    group = "Cluster",
    color = "Cluster", size = 1, dot.size = 3,
    palette = "aas",
    add = "segment",
    position = position_dodge(0),
    #sorting = "descending",
    #facet.by = "Cluster",
    #rotate = TRUE,
    #legend = "none",
    ggtheme = theme_bw(base_size = 14, base_family = "serif"),
    xlab = "Fonctions Cognitives",
    ylab = "Moyennes",
    title =
      "Scores aux differentes fonctions cognitives en fonction des clusters"
    ) +
  geom_smooth(aes(group = Cluster, color = Cluster),size = .8) +
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        )

# ---- object_img_violins ------------------------------------------------------
# Comparaisons de moyennes entre clusters par variable

# Object mean comparison
data %>% 
  ggplot(aes(x = Cluster,reorder(1, 2, 3, 4),
             y = OSIQ_O,
             fill = Cluster,)) +
  geom_violin(alpha = 0.3,
              position = position_dodge(1),
              draw_quantiles = FALSE,) +
  geom_boxplot(alpha = 0.6,
               position = position_dodge(1),
               width = 0.15,) +
  # significance indicator labels
  stat_compare_means(comparisons = list(c("1","2"),
                                        c("3","4"), 
                                        c("1","4")
  ),
  method = "t.test",
  label = "p.format",
  # height of the labels
  label.y.npc = c("top")) +
  # different filler colors
  scale_fill_brewer(palette = "Dark2") +
  scale_colour_brewer(palette = "Dark2") +
  labs(title = "Distribution des scores d'imagerie visuelle-objet par cluster",
       x = "Cluster",
       y = "Score à l'OSIQ Object Scale")

#---- spatial_img_violins ------------------------------------------------------
# Spatial mean comparison
data %>% 
  ggplot(aes(x = Cluster,reorder(1, 2, 3, 4),
             y = OSIQ_S,
             fill = Cluster,)) +
  geom_violin(alpha = 0.3,
              position = position_dodge(1),
              draw_quantiles = FALSE,) +
  geom_boxplot(alpha = 0.6,
               position = position_dodge(1),
               width = 0.15,) +
  # significance indicator labels
  stat_compare_means(comparisons = list(c("1","2"),
                                        c("2","3"), 
                                        c("3","4"),
                                        c("1","4")
  ),
  method = "t.test",
  label = "p.format",
  # height of the labels
  label.y.npc = c("top")) +
  # different filler colors
  scale_fill_brewer(palette = "Dark2") +
  scale_colour_brewer(palette = "Dark2") +
  labs(title = "Distribution des scores d'imagerie visuospatiale par cluster",
       x = "Cluster",
       y = "Score à l'OSIQ Spatial Scale")

```