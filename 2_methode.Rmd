# Expérience

## Méthode

### Participants

Nous recruterons des participants à partir de mi-février/début mars, lorsque l'expérience sera codée et prête à être diffusée en ligne. Pour estimer un ordre de grandeur du nombre de participants nécessaire pour obtenir des résultats intéressants, nous pouvons nous baser sur les calculs de puissance de @dawesCognitiveProfileMultisensory2020, qui ont mené une étude assez proche de la nôtre impliquant douze questionnaires et ayant pour objectif de mieux cerner le profil des aphantasiques. Ceux-ci ont estimé que pour une taille d'effet modérée des comparaisons, une puissance de 80% et avec un $\alpha$ très conservateur de 0.0002 (voir la section *\nameref{analyses}*), au moins 170 participants seraient nécessaires par groupe expérimental.   

Les participants devront avoir entre 18 et 50 ans et être locuteurs natifs français^[Il est à noter que dans l'étude de Dawes et al. [-@dawesCognitiveProfileMultisensory2020], 31 pays de résidence ont été répertoriés, avec 83% (*N* = 220) déclarant que l'anglais était leur première langue, et 88% (*N* = 235), s'identifiant comme blancs/caucasiens. Les résultats sont néanmoins cohérent avec le reste de la littérature sur l'aphantasie, avec aucun effet du langage. Cette étude interroge sur le potentiel intérêt de tenter de diffuser la présente étude à l'international.], avoir une vision normale ou corrigée et ne pas présenter de trouble de la lecture. Les participants aphantasiques seront recrutés en ligne sur des espaces spécifiques à leur condition (forums, groupes sur les réseaux sociaux, etc.). Nous nous intéressons à l'étude de l'aphantasie congénitale, les participants ne devront donc pas présenter d'antécédents de maladies neurologiques ou psychiatriques.      

Le critère répandu dans les études sur l'aphantasie pour identifier la condition est celui de l'auto-évaluation par le questionnaire VVIQ (Vividness of Visual Imagery Questionnaire, @marksVividnessVisualImagery1973), avec un score inférieur à 32 (voir la section *\nameref{questionnaires}*). Nous disposerons aussi de la composante objet de l'OSIQ (Object and Spatial Imagery Questionnaire, @blajenkovaObjectspatialImageryNew2006), fortement corrélée au VVIQ, fournissant donc des items supplémentaires - le seuil inférieur de ce questionnaire est évalué à 36, sur la base de deux écart-types à la moyenne de l'échantillon de @blajenkovaObjectspatialImageryNew2006. L'OSIQ, dans sa composante spatiale, sera par ailleurs utilisé pour l'auto-évaluation des capacités d'imagerie visuospatiales des participants, justifiant son utilisation en parallèle du VVIQ.   

En l'absence de données réelles de participants, nous avons donc préparé nos analyses prévisionnelles sur des données simulées sur R [@R-base]. Les paramètres de cette simulation (les résultats potentiels de chaque groupe) ont été basés sur la littérature et sur nos hypothèses : nous définirons donc le protocole dans un premier temps, puis reviendrons sur cette simulation dans la *section \ref{simulation}* dédiée.

### Équipement et procédure

Le protocole sera composé de plusieurs questionnaires et tâches qui seront administrées en ligne via un serveur JATOS [@langeJustAnotherTool2015]. Les questionnaires seront codés sur [SurveyJS](https://surveyjs.io/), une bibliothèque JavaScript Open Source dédiée à la création de questionnaires, et les tâches seront codées sur OpenSesame [@mathotOpenSesameOpensourceGraphical2012], une interface graphique de construction d'expériences comportementales. Avant les premiers questionnaires seront recueillies des données démographiques (âge, genre, métier et/ou études). Tous les participants devront donner leur consentement éclairé avant de commencer l'étude. La participation sera volontaire et sans compensation.

### Questionnaires

#### *Vividness of Visual Imagery Questionnaire* (VVIQ). {-}
Dawes 2022
The Vividness of Visual Imagery Questionnaire (VVIQ; Marks, 1995) is a 16-item scale which asks participants to imagine a person as well as several scenes and rate the vividness of these mental images using a 5-point scale ranging from 1 (“No image at all, you only “know” that you are thinking of the object”) to 5 (“Perfectly clear and <as> vivid as normal vision”). A single mean score on the VVIQ was computed for each participant. 
Une faible capacité d'imagerie visuelle est généralement définie par un score total de 32 ou moins au questionnaire sur la vivacité de l'imagerie visuelle (VVIQ : voir Questionnaires sur l'imagerie dans les documents), une échelle d'auto-évaluation de Likert en cinq points qui varie de 16 à 80 (Marks, 1995 ; Zeman et al., 2015). Un score total de 32 équivaut à une note de 2 ("vague et faible") pour chaque item du questionnaire ; où 1 = "Pas d'image du tout, vous savez seulement que vous pensez à l'objet"). 

Sema
Questionnaire de Vivacité de l'Imagerie Visuelle (Marks, 1973), 
qui comporte 16 items et dans lequel le participant doit coter sur une échelle de Likert de 5 points l'énoncé qui lui correspond le mieux. 
Les scores vont de 16 à 80. 
Il constitue le questionnaire subjectif de référence dans l'aphantasie. 

#### *Object and Spatial Imagery Questionnaire* (OSIQ). {-}

OSIQ
The Object and Spatial Imagery Questionnaire (OSIQ; Blajenkova, Kozhevnikov, & Motes, 2006) is a 30-item scale which requires participants to indicate how well each of several statements on object imagery ability (e.g. “When I imagine the face of a friend, I have a perfectly clear and bright image”) and spatial imagery ability (e.g. “I am a good Tetris player”) applies to them on a 5-point scale ranging from 1 (“Totally disagree”) to 5 (“Totally agree”). There are 15 items each comprising the Object and Spatial imagery domains of the OSIQ, averaged to form a mean score on each domain.

#### *Metacognition Awareness Inventory* (MAI). {-}
Le second questionnaire était le MAI, Inventaire de Conscience Métacognitive (Schraw & Dennison, 1994)
évalue les deux composantes de la métacognition : les connaissances métacognitives et la régulation métacognitive.

### Tâches

#### Matrices de Raven. {-}
Sema
version courte des matrices de Raven (Bilker et al., 2012)
capacités de raisonnement visuel et d'induction et de déduction de règles (

#### Tâche de Wason.{-}
Tâche de Wason, 1968
courte tâche de raisonnement

#### Sous-test des Similitudes de la WAIS-IV. {-}
les capacités d'abstraction et de conceptualisation verbale
Subtest Similitudes de la WAIS-IV 

#### Empan de chiffres envers. {-}
Empan de chiffre envers
la mémoire de travail verbale, 

#### *Wisconsin Card-Sorting Test*. {-}
les fonctions exécutives
WCST

#### Compréhension en lecture.{-}
et les capacités de compréhension en lecture
Textes. Cette dernière épreuve constituait une tache écologique dans laquelle les images mentales pouvaient être sollicitées. 
Tâche de lecture
Concernant la tâche de lecture, les participants étaient soumis à un texte qu'ils devaient lire. Le temps de lecture est libre mais chronométré. A la fin de la lecture, le texte est caché. Les participants devaient ensuite répondre à des questions sur le texte qui étaient d'ordres explicite ou implicite.

#### Blocs de Corsi. {-}

#### Test de Rotation Mentale (MRT). {-}

#### Spatial Reasoning Inventory (SRI). {-}

## Variables

Nos participants seront initialement divisés en deux groupes, et potentiellement subdivisés par la suite. Le ***Groupe*** sera donc notre seule **variable indépendante**. **Nos variables dépendantes** seront donc toutes nos mesures : i.e. les ***Scores*** au VVIQ, à l'OSIQ, au MAI, aux Matrices, aux Similitudes, aux textes de compréhension, au WCST, au MRT, au SRI, la précision au Wason, l'empan mnésique de chiffres, et le nombre de blocs rappelés au Corsi (qui correspond à un empan mnésique spatial) - soit treize scores.

## Hypothèses

### Imagerie visuelle-objet
Dawes 2022
Object Imagery
We expected aphantasic individuals to report reduced visual imagery ability compared to controls, in line with previous findings (Keogh & Pearson, 2018; Zeman et al., 2015). There is some suggestion that auditory imagery may also be reduced in individuals who report visual imagery absence, however this evidence comes from case studies with limited sample sizes (Greenberg & Knowlton, 2014). We therefore had no strong hypotheses regarding group differences in other multi-sensory imagery domains.

### Imagerie visuospatiale
Spatial Imagery
Lastly, we expected aphantasic self-reports of spatial imagery and spatial navigation abilities to align with data from previous studies suggesting that despite visual imagery absence, spatial abilities (as measured by questionnaires and performance on mental rotation and visuo-spatial tasks) appear to be largely preserved in aphantasia (Keogh & Pearson, 2018; Zeman et al., 2010).

### Raisonnement
D'après les données présentées par l'étude de Zeman et al. (2020) (taux important d'aphantasiques dans les métiers scientifiques), on peut faire l'hypothèse que le groupe de participants aphantasiques présentera des capacités de raisonnement (mesurées par le test des Similitudes et les Matrices) plus développés que le groupe de participants non aphantasiques.

### Compréhension en lecture
Dans la mesure où les aphantasiques ont un défaut d'imagerie visuelle, si le texte de compréhension en lecture sollicite des images visuelles, on peut s'attendre à des performances différentes à ce texte entre les aphantasiques et les non aphantasiques.

### Fonctions exécutives
Conformément à l'hypothèse que les aphantasiques auraient recours à des stratégies pour compenser leur déficit en imagerie visuelle, des performances élevées en compréhension de texte pourraient être corrélées à des scores élevés aux épreuves mesurant le fonctionnement exécutif et les capacités d'abstraction.

## Simulation

En suivant les recommandations des analyses de puissance, nous avons décidé de simuler *N* = 200 participants pour chaque groupe, aphantasiques et non-aphantasiques. Pour simuler ceux-ci, nous avons créé une **matrice des moyennes** et des écarts types arbitraires aux douze tâches et questionnaires pour chaque groupe, sur la base de la littérature et de nos hypothèses (avec donc une part inévitable de *wishful thinking* et de prophétie auto-réalisatrice). Ainsi nous avons fixé des moyennes aux tâches d'imagerie objet faibles chez les aphantasiques et hautes chez les contrôles, des scores aux tâches d'imagerie spatiale et de raisonnement légèrement plus élevés chez les aphantasiques, et des scores aux tâches de fonctions exécutives variables.      
Nous avons ensuite établi un **modèle de mesures**, une matrice définissant par des coefficients les liens entre nos douze variables et ce qu'elles "évaluent vraiment", les capacités cognitives sous-jacentes. Nous avons choisi d'en désigner cinq : l'*imagerie objet*, l'*imagerie spatiale*, le *raisonnement abstrait*, la *mémoire de travail* et *la flexibilité mentale* (ces deux dernières pouvant être regroupées ou non sous la catégorie de *fonctions exécutives*). Nous aurons donc des corrélations entre nos variables évaluées, qu'il faudra éclaircir pour comprendre les aspects fondamentaux qu'elles révèlent.       
Enfin, nous avons fixé une **matrice de covariance** entre ces cinq capacités cognitives, qui sont loin d'être indépendantes : la littérature pointe par exemple vers des liens entre imagerie spatiale et raisonnement [@kozhevnikovSpatialVisualizationPhysics2007; @kozhevnikovTradeoffObjectSpatial2010], ou encore les différentes imageries et la mémoire de travail [@dawesCognitiveProfileMultisensory2020; @knightMemoryImageryNo2022; @salwayVisuospatialWorkingMemory1995]. Nous avons donc pondéré ces liens avec des coefficients arbitraires sur cette base et celle de nos prédictions.      
Notre fonction de simulation a donc eu pour tâche, à l'aide de ces trois matrices (*moyennes*, *mesures*, *covariance*), de créer des moyennes aléatoires -bien que liées par les corrélations sous-jacentes- pour chaque tâche et chaque participant, avec une distribution normale et l'ajout d'erreurs aléatoires normales. Les moyennes ont ensuite été standardisées en *z-scores* pour les analyses (et pour les rassembler si nécessaire). La fonction de simulation a été codée sur R [@R-base] (voir *\nameref{annexes}* pour la liste complète des packages cités dans le document, leurs références ainsi que les liens vers le code détaillé ici).

## Analyses                                   

### Transformation des données

Les analyses prévisionnelles ont de même été réalisées sur R.      
Dans la littérature, les mesures réelles de tâches comparables aux nôtres ont des distributions non-normales [@dawesCognitiveProfileMultisensory2020; @dawesInnerVisionsMind2022; @palermoCongenitalLackExtraordinary2022]. Après vérification des distributions par des tests de Shapiro-Wilk, nous pourrons dans ce cas réaliser des **tests non-paramétriques** tels que des tests de Kruskal-Wallis ou Mann-Whitney-U. Alternativement, nous pourrions réaliser des **transformations des données** pour les rapprocher de la normalité, de type Box-Cox par exemple. L'étude de Dawes et al. [-@dawesCognitiveProfileMultisensory2020] utilise une autre transformation centrée sur la médiane permettant de comparer les différences entre groupes pour chaque tâche selon la même échelle :
$$
y = \frac{x - (S.min + \frac{S.max -S.min}{2})}{S.max-S.min}
$$
Où y est le score transformé, x le score brut, S.min le score minimum et S.max le maximum. Dans les présentes analyses nous avons choisi la deuxième solution, en standardisant les scores en z-scores via la fonction `scale()` sur R, puis en les ramenant sur une échelle de 0 à 1, avec une médiane à 0.5, via la fonction `rescale()` du package `datawizard` sur R - de sorte à pouvoir construire des profils plus aisément. Nous n'avons pas eu besoin ici d'utiliser une transformation normale, car les données ont déjà été simulées comme tel.

### Composantes principales et *clustering*

Comme nous l'avons mentionné dans la section *\nameref{simulation}*, nos variables initiales (les scores) ont pour certaines des corrélations très significatives entre elles du fait de la proximité des capacités cognitives qu'elles évaluent. Nous pourrons alors réduire le nombre de variables en isolant des dimensions essentielles qu'elles représentent par une **analyse en composantes principales** (ACP). Celle-ci nous permettra alors de combiner les scores de différentes tâches pour les ramener à des scores liés à des capacités cognitives (e.g. un score en imagerie visuelle au lieu de deux scores au VVIQ et à l'OSIQ-objet). Cette analyse a ici été conduite sur R via la fonction `fviz_pca_var()` du package `factoextra`.

De même, nos groupes initiaux ont été définis de manière arbitraire (une limite de score définie par convention, VVIQ < 32) et pourraient représenter une division imprécise des participants. Pour corriger ce biais potentiel nous réaliserons une **analyse de partition non-supervisée** (dite en *clusters*") par l'algorithme des *k-means*, pour ainsi étudier la répartition en groupes qu'il propose en prenant en compte toutes nos variables redéfinies par l'ACP. L'algorithme fonctionne sur la base d'une *matrice de dissimilarité* selon des distances euclidiennes : i.e. il évalue "géométriquement" selon les axes de nos variables la "distance" entre chaque observation (ici les participants). La distance euclidienne en deux dimensions se calcule simplement par le théorème de Pythagore. Pour un nombre plus grand de dimensions, la formule généralisée est la suivante :
$$
D_{i,j}^2 = \sum_{v=1}^{n}(x_{vi}-x_{vj})^2
$$
"D" étant la distance entre i et j dans n dimensions, égale à la somme des carrés des distances dans chaque dimension. Cette distance, ou *"dissimilarité"*, une fois calculée pour toutes les observations (participants) permet d'obtenir une matrice de des distances entre chacune d'elles. Par suite, l'algorithme des *k-means* utilise cette matrice pour diviser les observations en *k* sous-groupes : il rassemble les observations les plus proches entre elles de sorte à minimiser la superposition entre les clusters, i.e. les observations pouvant se trouver dans plusieurs groupes définis. La détermination de *k* - i.e. le nombre de sous-groupes (ou *clusters*) idéal pour une partition intéressante - est une étape importante, et peut se réaliser via de nombreux indices. Nous avons ici utilisé la fonction `fviz_nbclust()` du package `factoextra` sur R. L'analyse en *clusters* elle-même a été conduite avec la fonction `kmeans()` du package `stats` et visualisée avec `fviz_cluster()` de `factoextra`.

### Composition des clusters et profils cognitifs

Pour finir, l'algorithme des *k-means* permettra donc de créer des groupes qui auront des profils particuliers sur chaque composante cognitive représentée par nos variables. La composition de ces groupes (en pourcentage d'aphantasiques/non-aphantasiques définis initialement) ainsi que leurs capacités cognitives seront analysées : la variable du *groupe* étant notre seule variable indépendante, nous réaliserons alors des **ANOVAs univariées** en fonction du groupe ainsi que des **t-tests post-hoc**. Nous pourrions par ailleurs ajuster des **modèles mixtes linéaires** sur nos données pour comparer la qualité d'un modèle à deux groupes ou à *k* groupes, ainsi que l'importance des facteurs aléatoires tels que les différences entre participants.        
Pour nos comparaisons, nous devrons choisir une correction pour compenser les tests multiples, la plus utilisée étant la **correction de Bonferroni**. Dawes et al. [-@dawesCognitiveProfileMultisensory2020] en ont utilisée une très conservatrice pour ajuster l'$\alpha$ en fonction de leur nombre d'items dans les questionnaires, donnant $\alpha$ = 0.05/206 = 0.0002. Palermo et al. [-@palermoCongenitalLackExtraordinary2022] ont utilisé une correction de $\alpha$ = 0.05/11 = 0.005 pour les ANOVAs, et de $\alpha$ = 0.05/6 = 0.008 pour les comparaisons post-hoc. Celle que nous utiliserons et le critère pour la choisir restent à définir.
